{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNjfhZhJHtOyL9xReqEASK/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##1. GPT-2 pre-trained weights 파일 로드 경로 셀"],"metadata":{"id":"aRiemulzSkLZ"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v-x0UYx5Up0C","executionInfo":{"status":"ok","timestamp":1770803306215,"user_tz":-540,"elapsed":42667,"user":{"displayName":"‍하준서(학부생-인공지능전공)","userId":"02801132016592520542"}},"outputId":"2809973a-5209-4b6f-84bb-401d44ff825c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","BASE: /content/drive/MyDrive/Colab Notebooks/GPT-2_numpy_Quantized_TF_inference\n","MODEL_PATH: /content/drive/MyDrive/Colab Notebooks/GPT-2_numpy_Quantized_TF_inference/model.safetensors\n"]}],"source":["!pip -q install regex safetensors\n","\n","import os, json\n","import regex as re\n","import numpy as np\n","from safetensors.numpy import load_file\n","from google.colab import drive\n","from pathlib import Path\n","\n","drive.mount(\"/content/drive\")\n","\n","def pick_base():\n","    required = [\"model.safetensors\", \"config.json\", \"vocab.json\", \"merges.txt\"]\n","\n","    # 1) 현재 작업 폴더(/content 등)에서 먼저 찾기\n","    here = Path(\".\").resolve()\n","    if all((here / f).exists() for f in required):\n","        return here\n","\n","    # 2) Drive 안에서 model.safetensors 위치를 찾아 BASE로 사용\n","    drive_root = Path(\"/content/drive/MyDrive\")\n","    hits = list(drive_root.rglob(\"model.safetensors\"))\n","    if not hits:\n","        raise FileNotFoundError(\"Drive(MyDrive)에서 model.safetensors를 찾지 못했습니다. 파일이 Drive에 있는지 확인하세요.\")\n","\n","    base = hits[0].parent  # 첫 번째 발견 폴더를 BASE로 사용\n","    # 나머지 필수 파일도 같은 폴더에 있는지 확인\n","    for f in required:\n","        if not (base / f).exists():\n","            raise FileNotFoundError(f\"'{base}' 폴더에 {f} 가 없습니다. 4개 파일을 같은 폴더에 두세요.\")\n","    return base\n","\n","BASE = pick_base()\n","\n","MODEL_PATH  = BASE / \"model.safetensors\"\n","CONFIG_PATH = BASE / \"config.json\"\n","VOCAB_PATH  = BASE / \"vocab.json\"\n","MERGES_PATH = BASE / \"merges.txt\"\n","\n","print(\"BASE:\", BASE)\n","print(\"MODEL_PATH:\", MODEL_PATH)\n"]},{"cell_type":"markdown","source":["## 2. 수식 구현 함수 셀"],"metadata":{"id":"yJxn0aFoSt8g"}},{"cell_type":"code","source":["# =========================================================\n","# 0) GPT-2 BPE Tokenizer (pure python)\n","# =========================================================\n","\n","def bytes_to_unicode():\n","    \"\"\"\n","    GPT-2 byte encoder mapping.\n","    Maps bytes (0..255) to unicode chars to make reversible BPE.\n","    \"\"\"\n","    bs = list(range(ord(\"!\"), ord(\"~\")+1)) + list(range(ord(\"¡\"), ord(\"¬\")+1)) + list(range(ord(\"®\"), ord(\"ÿ\")+1))\n","    cs = bs[:]\n","    n = 0\n","    for b in range(256):\n","        if b not in bs:\n","            bs.append(b)\n","            cs.append(256 + n)\n","            n += 1\n","    cs = [chr(c) for c in cs]\n","    return dict(zip(bs, cs))\n","\n","BYTE_ENCODER = bytes_to_unicode()\n","BYTE_DECODER = {v: k for k, v in BYTE_ENCODER.items()}\n","\n","# This regex is the standard GPT-2 pattern (as used in many implementations)\n","PATTERN = re.compile(\n","    r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\",\n","    re.IGNORECASE\n",")\n","\n","def get_pairs(word):\n","    # word: tuple of symbols\n","    pairs = set()\n","    prev = word[0]\n","    for ch in word[1:]:\n","        pairs.add((prev, ch))\n","        prev = ch\n","    return pairs\n","\n","class GPT2BPETokenizer:\n","    def __init__(self, vocab_json_path: str, merges_txt_path: str):\n","        with open(vocab_json_path, \"r\", encoding=\"utf-8\") as f:\n","            self.encoder = json.load(f)              # token(str) -> id(int)\n","        self.decoder = {v: k for k, v in self.encoder.items()}  # id -> token(str)\n","\n","        with open(merges_txt_path, \"r\", encoding=\"utf-8\") as f:\n","            merges = f.read().splitlines()\n","        merges = merges[1:]  # first line is a header\n","        merges = [tuple(m.split()) for m in merges if m and not m.startswith(\"#\")]\n","        self.bpe_ranks = {pair: i for i, pair in enumerate(merges)}\n","\n","        self.cache = {}\n","\n","        # special token\n","        self.eos_token_id = 50256\n","\n","    def bpe(self, token: str):\n","        if token in self.cache:\n","            return self.cache[token]\n","\n","        word = tuple(token)\n","        pairs = get_pairs(word)\n","\n","        if not pairs:\n","            self.cache[token] = token\n","            return token\n","\n","        while True:\n","            bigram = min(pairs, key=lambda p: self.bpe_ranks.get(p, 10**10))\n","            if bigram not in self.bpe_ranks:\n","                break\n","\n","            first, second = bigram\n","            new_word = []\n","            i = 0\n","            while i < len(word):\n","                try:\n","                    j = word.index(first, i)\n","                    new_word.extend(word[i:j])\n","                    i = j\n","                except ValueError:\n","                    new_word.extend(word[i:])\n","                    break\n","\n","                if i < len(word)-1 and word[i] == first and word[i+1] == second:\n","                    new_word.append(first + second)\n","                    i += 2\n","                else:\n","                    new_word.append(word[i])\n","                    i += 1\n","\n","            word = tuple(new_word)\n","            if len(word) == 1:\n","                break\n","            pairs = get_pairs(word)\n","\n","        out = \" \".join(word)\n","        self.cache[token] = out\n","        return out\n","\n","    def encode(self, text: str):\n","        # 1) split into \"words\"\n","        tokens = []\n","        for m in re.finditer(PATTERN, text):\n","            piece = m.group(0)\n","            # 2) byte encode -> unicode string\n","            piece_bytes = piece.encode(\"utf-8\")\n","            piece_trans = \"\".join(BYTE_ENCODER[b] for b in piece_bytes)\n","            # 3) bpe\n","            bpe_tokens = self.bpe(piece_trans).split(\" \")\n","            # 4) map to ids\n","            tokens.extend(self.encoder[t] for t in bpe_tokens)\n","        return tokens\n","\n","    def decode(self, token_ids):\n","        # token_ids -> concatenated token strings -> byte decode\n","        text = \"\".join(self.decoder[t] for t in token_ids)\n","        byte_arr = [BYTE_DECODER[c] for c in text]\n","        return bytes(byte_arr).decode(\"utf-8\", errors=\"replace\")\n","\n","\n","# =========================================================\n","# 1) Config (GPT-2)\n","# =========================================================\n","class GPT2Config:\n","    def __init__(self, vocab_size=50257, n_layers=12, d_model=768, n_heads=12, d_ff=3072, max_seq=1024, ln_eps=1e-5):\n","        assert d_model % n_heads == 0\n","        self.vocab_size = vocab_size\n","        self.n_layers = n_layers\n","        self.d_model = d_model\n","        self.n_heads = n_heads\n","        self.d_head = d_model // n_heads\n","        self.d_ff = d_ff\n","        self.max_seq = max_seq\n","        self.ln_eps = ln_eps\n","\n","\n","# =========================================================\n","# 2) Core ops\n","# =========================================================\n","def layernorm(x, g, b, eps=1e-5):\n","    mu = x.mean()\n","    var = ((x - mu) ** 2).mean()\n","    xhat = (x - mu) / np.sqrt(var + eps)\n","    return xhat * g + b\n","\n","def gelu_new(x):\n","    # GPT-2 approximation (same style you already used)\n","    return 0.5 * x * (1.0 + np.tanh(np.sqrt(2.0/np.pi) * (x + 0.044715 * (x**3))))\n","\n","def softmax_1d(logits):\n","    m = np.max(logits)\n","    exps = np.exp(logits - m)\n","    s = np.sum(exps)\n","    return exps / (s + 1e-12)\n","\n","def top_k_filter(logits, k):\n","    if k <= 0 or k >= logits.shape[0]:\n","        return logits\n","    out = logits.copy()\n","    idx = np.argpartition(out, -k)[:-k]\n","    out[idx] = -1e30\n","    return out\n","\n","def sample_from_probs(probs):\n","    r = np.random.rand()\n","    c = np.cumsum(probs)\n","    return int(np.searchsorted(c, r, side=\"right\"))\n","\n","# =========================================================\n","# ３）Quantization utils (W8A16)\n","# =========================================================\n","def quantize_w_int8(W: np.ndarray, axis=None):\n","    \"\"\"\n","    Symmetric INT8 quantization.\n","    axis=None  : per-tensor\n","    axis=0/1   : per-channel (선택)\n","    Returns: (W_q:int8, scale:float32 or float32 array)\n","    \"\"\"\n","    W = W.astype(np.float32)\n","\n","    if axis is None:\n","        s = np.max(np.abs(W)) / 127.0 + 1e-12\n","        Wq = np.round(W / s).astype(np.int8)\n","        return Wq, np.float32(s)\n","\n","    # per-channel\n","    maxv = np.max(np.abs(W), axis=axis, keepdims=True)\n","    s = maxv / 127.0 + 1e-12\n","    Wq = np.round(W / s).astype(np.int8)\n","    return Wq, s.astype(np.float32)\n","\n","\n","def make_int8_weights(W_fp32: dict):\n","    \"\"\"\n","    Build INT8 weights (W8A16).\n","    - Keep embeddings & LN params in FP32\n","    - Quantize Linear weights only\n","    \"\"\"\n","    W8 = {}\n","    W8[\"tok_emb\"] = W_fp32[\"tok_emb\"]      # FP32 유지\n","    W8[\"pos_emb\"] = W_fp32[\"pos_emb\"]      # FP32 유지\n","    W8[\"ln_f_g\"]  = W_fp32[\"ln_f_g\"]\n","    W8[\"ln_f_b\"]  = W_fp32[\"ln_f_b\"]\n","\n","    W8[\"blocks\"] = []\n","    for blk in W_fp32[\"blocks\"]:\n","        b = {}\n","\n","        # LN / bias는 FP32 유지\n","        b[\"ln1_g\"] = blk[\"ln1_g\"]; b[\"ln1_b\"] = blk[\"ln1_b\"]\n","        b[\"ln2_g\"] = blk[\"ln2_g\"]; b[\"ln2_b\"] = blk[\"ln2_b\"]\n","        b[\"bqkv\"]  = blk[\"bqkv\"]\n","        b[\"bo\"]    = blk[\"bo\"]\n","        b[\"b1\"]    = blk[\"b1\"]\n","        b[\"b2\"]    = blk[\"b2\"]\n","\n","        # ---- Quantize Linear weights (per-tensor부터 시작) ----\n","        b[\"Wqkv_q\"], b[\"Wqkv_s\"] = quantize_w_int8(blk[\"Wqkv\"], axis=None)\n","        b[\"Wo_q\"],   b[\"Wo_s\"]   = quantize_w_int8(blk[\"Wo\"],   axis=None)\n","        b[\"W1_q\"],   b[\"W1_s\"]   = quantize_w_int8(blk[\"W1\"],   axis=None)\n","        b[\"W2_q\"],   b[\"W2_s\"]   = quantize_w_int8(blk[\"W2\"],   axis=None)\n","\n","        W8[\"blocks\"].append(b)\n","\n","    return W8\n","\n","def make_fp32_as_int8dict(W_fp32: dict):\n","    \"\"\"\n","    FP32 weights를 INT8-dict 인터페이스로 '포장'한다.\n","    - Wqkv_q/Wo_q/W1_q/W2_q에 FP32 weight를 그대로 넣고\n","    - scale은 1.0으로 둬서 결과가 FP32와 동일하게 나오게 함\n","    \"\"\"\n","    W2 = {}\n","    W2[\"tok_emb\"] = W_fp32[\"tok_emb\"]\n","    W2[\"pos_emb\"] = W_fp32[\"pos_emb\"]\n","    W2[\"ln_f_g\"]  = W_fp32[\"ln_f_g\"]\n","    W2[\"ln_f_b\"]  = W_fp32[\"ln_f_b\"]\n","\n","    W2[\"blocks\"] = []\n","    for blk in W_fp32[\"blocks\"]:\n","        b = {}\n","        # LN / bias는 그대로\n","        b[\"ln1_g\"] = blk[\"ln1_g\"]; b[\"ln1_b\"] = blk[\"ln1_b\"]\n","        b[\"ln2_g\"] = blk[\"ln2_g\"]; b[\"ln2_b\"] = blk[\"ln2_b\"]\n","        b[\"bqkv\"]  = blk[\"bqkv\"]\n","        b[\"bo\"]    = blk[\"bo\"]\n","        b[\"b1\"]    = blk[\"b1\"]\n","        b[\"b2\"]    = blk[\"b2\"]\n","\n","        # \"q\" 자리에 FP32 weight 그대로, scale=1.0\n","        b[\"Wqkv_q\"] = blk[\"Wqkv\"].astype(np.float32); b[\"Wqkv_s\"] = np.float32(1.0)\n","        b[\"Wo_q\"]   = blk[\"Wo\"].astype(np.float32);   b[\"Wo_s\"]   = np.float32(1.0)\n","        b[\"W1_q\"]   = blk[\"W1\"].astype(np.float32);   b[\"W1_s\"]   = np.float32(1.0)\n","        b[\"W2_q\"]   = blk[\"W2\"].astype(np.float32);   b[\"W2_s\"]   = np.float32(1.0)\n","\n","        W2[\"blocks\"].append(b)\n","\n","    return W2\n","\n","\n","\n","# =========================================================\n","# ４) KV Cache (same idea, works for GPT-2 incremental decoding)\n","# =========================================================\n","class KVCache:\n","    \"\"\"\n","    For each layer:\n","      K: (T, H, Dh)\n","      V: (T, H, Dh)\n","    \"\"\"\n","    def __init__(self, cfg: GPT2Config):\n","        self.cfg = cfg\n","        self.K = [None for _ in range(cfg.n_layers)]\n","        self.V = [None for _ in range(cfg.n_layers)]\n","        self.T = 0\n","\n","    def append(self, layer, k_h, v_h):\n","        assert 0 <= layer < self.cfg.n_layers\n","        assert self.T < self.cfg.max_seq\n","        assert k_h.shape == (self.cfg.n_heads, self.cfg.d_head)\n","        assert v_h.shape == (self.cfg.n_heads, self.cfg.d_head)\n","\n","        if self.K[layer] is None:\n","            self.K[layer] = k_h[None, :, :]\n","            self.V[layer] = v_h[None, :, :]\n","        else:\n","            self.K[layer] = np.concatenate([self.K[layer], k_h[None, :, :]], axis=0)\n","            self.V[layer] = np.concatenate([self.V[layer], v_h[None, :, :]], axis=0)\n","\n","    def step_done(self):\n","        self.T += 1\n","\n","\n","# =========================================================\n","# ５) Load GPT-2 weights from model.safetensors\n","#    and map into a numpy-friendly dict\n","# =========================================================\n","def load_gpt2_weights_from_safetensors(model_safetensors_path: str):\n","    \"\"\"\n","    Returns a dict W with:\n","      tok_emb: (V, D)\n","      pos_emb: (max_seq, D)\n","      blocks: list of per-layer dict:\n","        ln1_g, ln1_b\n","        Wqkv, bqkv\n","        Wo, bo\n","        ln2_g, ln2_b\n","        W1, b1\n","        W2, b2\n","      ln_f_g, ln_f_b\n","    We tie output head to tok_emb (standard GPT-2).\n","    \"\"\"\n","    T = load_file(model_safetensors_path)\n","\n","    W = {}\n","    W[\"tok_emb\"] = T[\"wte.weight\"].astype(np.float32)   # (V, D)\n","    W[\"pos_emb\"] = T[\"wpe.weight\"].astype(np.float32)   # (max_seq, D)\n","\n","    # blocks\n","    blocks = []\n","    i = 0\n","    while f\"h.{i}.ln_1.weight\" in T:\n","        blk = {}\n","        blk[\"ln1_g\"] = T[f\"h.{i}.ln_1.weight\"].astype(np.float32)\n","        blk[\"ln1_b\"] = T[f\"h.{i}.ln_1.bias\"].astype(np.float32)\n","\n","        # fused qkv\n","        blk[\"Wqkv\"] = T[f\"h.{i}.attn.c_attn.weight\"].astype(np.float32)  # (D, 3D)\n","        blk[\"bqkv\"] = T[f\"h.{i}.attn.c_attn.bias\"].astype(np.float32)    # (3D,)\n","\n","        blk[\"Wo\"] = T[f\"h.{i}.attn.c_proj.weight\"].astype(np.float32)    # (D, D)\n","        blk[\"bo\"] = T[f\"h.{i}.attn.c_proj.bias\"].astype(np.float32)      # (D,)\n","\n","        blk[\"ln2_g\"] = T[f\"h.{i}.ln_2.weight\"].astype(np.float32)\n","        blk[\"ln2_b\"] = T[f\"h.{i}.ln_2.bias\"].astype(np.float32)\n","\n","        blk[\"W1\"] = T[f\"h.{i}.mlp.c_fc.weight\"].astype(np.float32)       # (D, 4D)\n","        blk[\"b1\"] = T[f\"h.{i}.mlp.c_fc.bias\"].astype(np.float32)         # (4D,)\n","        blk[\"W2\"] = T[f\"h.{i}.mlp.c_proj.weight\"].astype(np.float32)     # (4D, D)\n","        blk[\"b2\"] = T[f\"h.{i}.mlp.c_proj.bias\"].astype(np.float32)       # (D,)\n","\n","        blocks.append(blk)\n","        i += 1\n","\n","    W[\"blocks\"] = blocks\n","    W[\"ln_f_g\"] = T[\"ln_f.weight\"].astype(np.float32)\n","    W[\"ln_f_b\"] = T[\"ln_f.bias\"].astype(np.float32)\n","\n","    return W\n","\n","\n","# =========================================================\n","# ６) One-step forward (GPT-2 style: learned pos emb, fused QKV)\n","# =========================================================\n","def attention_step(cfg: GPT2Config, blk, x_ln, pos, cache: KVCache, layer: int):\n","    \"\"\"\n","    x_ln: (D,) already layernormed\n","    returns attention output (D,)\n","    \"\"\"\n","    D, H, Dh = cfg.d_model, cfg.n_heads, cfg.d_head\n","\n","    assert cache.T == pos  # incremental decoding\n","\n","    # fused qkv: (D,) @ (D, 3D) -> (3D,)\n","    Wqkv = blk[\"Wqkv_q\"].astype(np.float32) * blk[\"Wqkv_s\"]\n","    qkv = x_ln @ Wqkv + blk[\"bqkv\"]\n","    q, k, v = np.split(qkv, 3)\n","\n","    # reshape to heads: (H, Dh)\n","    q = q.reshape(H, Dh)\n","    k = k.reshape(H, Dh)\n","    v = v.reshape(H, Dh)\n","\n","    # append current token k,v to cache\n","    cache.append(layer, k.astype(np.float32), v.astype(np.float32))\n","\n","    K = cache.K[layer]  # (T+1, H, Dh)\n","    V = cache.V[layer]  # (T+1, H, Dh)\n","    Tlen = K.shape[0]\n","\n","    scale = 1.0 / np.sqrt(Dh)\n","\n","    out = np.zeros((H, Dh), dtype=np.float32)\n","    for h in range(H):\n","        scores = (K[:, h, :] @ q[h]) * scale   # (Tlen,)\n","        probs = softmax_1d(scores)\n","        out[h] = probs @ V[:, h, :]            # (Dh,)\n","\n","    out = out.reshape(D)\n","    Wo = blk[\"Wo_q\"].astype(np.float32) * blk[\"Wo_s\"]\n","    out = out @ Wo + blk[\"bo\"]\n","    return out.astype(np.float32)\n","\n","def ffn_step(cfg: GPT2Config, blk, x_ln2):\n","    W1 = blk[\"W1_q\"].astype(np.float32) * blk[\"W1_s\"]\n","    h  = x_ln2 @ W1 + blk[\"b1\"]\n","    h = gelu_new(h)\n","    W2 = blk[\"W2_q\"].astype(np.float32) * blk[\"W2_s\"]\n","    y = h @ W2 + blk[\"b2\"]       # (D,)\n","    return y.astype(np.float32)\n","\n","def gpt2_step(cfg: GPT2Config, W, token_id: int, pos: int, cache: KVCache):\n","    \"\"\"\n","    One token step forward; updates cache. Returns last hidden (D,)\n","    \"\"\"\n","    assert 0 <= token_id < cfg.vocab_size\n","    assert 0 <= pos < cfg.max_seq\n","\n","    # GPT-2 input: tok_emb + pos_emb\n","    x = (W[\"tok_emb\"][token_id] + W[\"pos_emb\"][pos]).astype(np.float32)\n","\n","    for layer in range(cfg.n_layers):\n","        blk = W[\"blocks\"][layer]\n","\n","        # LN1 -> Attn -> Residual (pre-LN)\n","        x_ln = layernorm(x, blk[\"ln1_g\"], blk[\"ln1_b\"], eps=cfg.ln_eps).astype(np.float32)\n","        a = attention_step(cfg, blk, x_ln, pos, cache, layer)\n","        x = (x + a).astype(np.float32)\n","\n","        # LN2 -> FFN -> Residual\n","        x_ln2 = layernorm(x, blk[\"ln2_g\"], blk[\"ln2_b\"], eps=cfg.ln_eps).astype(np.float32)\n","        f = ffn_step(cfg, blk, x_ln2)\n","        x = (x + f).astype(np.float32)\n","\n","    x = layernorm(x, W[\"ln_f_g\"], W[\"ln_f_b\"], eps=cfg.ln_eps).astype(np.float32)\n","\n","    cache.step_done()\n","    return x\n","\n","def logits_from_hidden(W, h):\n","    # GPT-2 typically ties lm_head weight to token embedding\n","    # logits = h @ W[\"tok_emb\"].T\n","    return (h @ W[\"tok_emb\"].T).astype(np.float32)\n","\n","# =========================================================\n","# ７) Generation\n","# =========================================================\n","def generate(cfg: GPT2Config, W, tokenizer: GPT2BPETokenizer, prompt: str,\n","             max_new_tokens=64, temperature=1.0, top_k=0, do_sample=False, stop_on_eos=True):\n","    tokens = tokenizer.encode(prompt)\n","    assert len(tokens) <= cfg.max_seq\n","\n","    cache = KVCache(cfg)\n","\n","    # prefill\n","    h = None\n","    for pos, tid in enumerate(tokens):\n","        h = gpt2_step(cfg, W, tid, pos, cache)\n","\n","    # generate loop\n","    for _ in range(max_new_tokens):\n","        logits = logits_from_hidden(W, h)\n","\n","        t = max(1e-6, float(temperature))\n","        logits = logits / t\n","        logits = top_k_filter(logits, top_k)\n","        probs = softmax_1d(logits)\n","\n","        if do_sample:\n","            next_id = sample_from_probs(probs)\n","        else:\n","            next_id = int(np.argmax(probs))\n","\n","        tokens.append(next_id)\n","\n","        if stop_on_eos and next_id == tokenizer.eos_token_id:\n","            break\n","\n","        pos = len(tokens) - 1\n","        if pos >= cfg.max_seq:\n","            break\n","        h = gpt2_step(cfg, W, next_id, pos, cache)\n","\n","    return tokenizer.decode(tokens), tokens\n","\n"],"metadata":{"id":"WNOr-XRgVMt2","executionInfo":{"status":"ok","timestamp":1770803306244,"user_tz":-540,"elapsed":24,"user":{"displayName":"‍하준서(학부생-인공지능전공)","userId":"02801132016592520542"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## 3. 추론 실행 셀"],"metadata":{"id":"QscOCjLSS-1h"}},{"cell_type":"code","source":["# =========================================================\n","# ８) Load GPT-2 & Run FP32/INT8 Generation\n","# =========================================================\n","\n","# 1) tokenizer\n","tokenizer = GPT2BPETokenizer(VOCAB_PATH, MERGES_PATH)\n","\n","# 2) raw FP32 weights (원본 그대로)\n","W_raw = load_gpt2_weights_from_safetensors(MODEL_PATH)\n","\n","# 3) config\n","with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n","    c = json.load(f)\n","\n","cfg = GPT2Config(\n","    vocab_size=c.get(\"vocab_size\", 50257),\n","    n_layers=c.get(\"n_layer\", 12),\n","    d_model=c.get(\"n_embd\", 768),\n","    n_heads=c.get(\"n_head\", 12),\n","    d_ff=4 * c.get(\"n_embd\", 768),\n","    max_seq=c.get(\"n_positions\", 1024),\n","    ln_eps=c.get(\"layer_norm_epsilon\", 1e-5),\n",")\n","\n","# FP32 비교용 \"포장\"\n","W_fp32 = make_fp32_as_int8dict(W_raw)\n","\n","# INT8 적용용 (반드시 raw에서 만들어야 함)\n","W_int8 = make_int8_weights(W_raw)\n","\n","prompt = \"Hello! This is a NumPy GPT-2. \"\n","\n","# FP32 (greedy)\n","out_fp32, toks_fp32 = generate(cfg, W_fp32, tokenizer, prompt,\n","                       max_new_tokens=40, do_sample=False)\n","\n","# INT8 (greedy)\n","out_int8, toks_int8 = generate(cfg, W_int8, tokenizer, prompt,\n","                       max_new_tokens=40, do_sample=False)\n","\n","print(\"FP32:\", out_fp32)\n","print(\"FP32 #tokens:\", len(toks_fp32))\n","\n","print(\"INT8:\", out_int8)\n","print(\"INT8 #tokens:\", len(toks_int8))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o-TT7d6nV72x","executionInfo":{"status":"ok","timestamp":1770803346515,"user_tz":-540,"elapsed":40267,"user":{"displayName":"‍하준서(학부생-인공지능전공)","userId":"02801132016592520542"}},"outputId":"0b52b30d-02c5-4e73-ced5-c6ec8ea77e06"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["FP32: Hello! This is a NumPy GPT-2.  I'm not sure if I'm going to use it, but I'm sure it's a good one.  I'm not sure if I'm going to use it, but I'm\n","FP32 #tokens: 53\n","INT8: Hello! This is a NumPy GPT-2.  I'm not sure if I'm going to be able to use it in my Python code, but I'm sure I can use it in my Python code.  I'm sure I can\n","INT8 #tokens: 53\n"]}]}]}